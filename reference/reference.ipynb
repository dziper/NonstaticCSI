{
 "cells": [
  {
   "cell_type": "code",
   "id": "15d584a57b4e0a7b",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-29T18:51:09.736625Z",
     "start_time": "2024-11-29T18:51:09.500088Z"
    }
   },
   "source": [
    "# Translated to Python from https://github.com/matteonerini/ml-based-csi-feedback\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.io import loadmat\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Parameters\n",
    "# na = 64               # Number of BS antennas\n",
    "# nc = 160              # Number of OFDM subcarriers\n",
    "# nTrain = 5000         # Number of training samples\n",
    "# nTest = 2000          # Number of test samples\n",
    "CR = 16               # Compression ratio\n",
    "BTot = 256 * 5        # Total feedback bits\n",
    "snrTrain = -1         # SNR for training in linear units\n",
    "snrTest = -1          # SNR for testing in linear units\n",
    "quantization = True   # Whether to quantize the compressed CSI\n",
    "reduce_overhead = True # Whether to reduce overhead\n",
    "our_data = True\n",
    "C = 250\n"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:51:10.704300Z",
     "start_time": "2024-11-29T18:51:09.739756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Load data\n",
    "if our_data:\n",
    "    print('Importing our data')\n",
    "    with open(\"../data/dataset1/batch1_1Lane_450U_1Rx_64Tx_1T_160K_2620000000.0fc.pickle\", 'rb') as f: # change file link for your machine\n",
    "        data = pickle.load(f)\n",
    "    freq_channel = np.array(data[\"freq_channel\"])\n",
    "    freq_channel = np.squeeze(freq_channel)\n",
    "    \n",
    "    indices = np.random.permutation(freq_channel.shape[0])\n",
    "    shuffled_data = freq_channel[indices]\n",
    "    \n",
    "    train_ratio = 0.8\n",
    "    split_index = int(len(shuffled_data) * train_ratio)\n",
    "    \n",
    "    # Increase the length of the training data\n",
    "    def duplicate_and_add_noise(data, times, noise_std_dev):\n",
    "        duplicated_data = np.repeat(data, times, axis=0)  # Duplicate data k times\n",
    "        noise = np.random.normal(0, noise_std_dev, duplicated_data.shape) + \\\n",
    "                1j * np.random.normal(0, noise_std_dev, duplicated_data.shape)\n",
    "        return duplicated_data + noise\n",
    "    \n",
    "    duplicate_times = 5\n",
    "    noise_std_dev = 1e-8\n",
    "    H_train = shuffled_data[:split_index]\n",
    "    H_train = duplicate_and_add_noise(H_train, duplicate_times, noise_std_dev)\n",
    "    H_train = np.transpose(H_train, (1,2,0))\n",
    "    \n",
    "    H_test = shuffled_data[split_index:]\n",
    "    H_test = duplicate_and_add_noise(H_test, duplicate_times, noise_std_dev)\n",
    "    H_test = np.transpose(H_test, (1,2,0))\n",
    "    \n",
    "else:\n",
    "    print('Importing Reference Data')\n",
    "    H_train = loadmat('H_train.mat')['H_train']\n",
    "    H_test = loadmat('H_test.mat')['H_test']\n",
    "\n",
    "na = H_train.shape[0]\n",
    "nc = H_train.shape[1]\n",
    "nTrain = H_train.shape[2]\n",
    "nTest = H_test.shape[2]\n",
    "\n",
    "assert BTot < nTrain, \"Larger BTot unsupported by func_allocate_bits\""
   ],
   "id": "3d99c6439d69db29",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:51:11.029119Z",
     "start_time": "2024-11-29T18:51:10.705003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('Preprocessing Data')\n",
    "# Uplink Training Data\n",
    "HUL_train_n = H_train.copy()\n",
    "Lambda = 1.0 / np.mean(np.abs(HUL_train_n)**2, axis=(0, 1))\n",
    "\n",
    "if snrTrain != -1:\n",
    "    nPower = 1.0 / (Lambda * snrTrain)\n",
    "    HN = (np.random.randn(na, nc, nTrain) + 1j * np.random.randn(na, nc, nTrain)) * \\\n",
    "         np.sqrt(nPower[np.newaxis, np.newaxis, :] / 2)\n",
    "    HUL_train_n += HN\n",
    "    Lambda = 1.0 / np.mean(np.abs(HUL_train_n)**2, axis=(0, 1))\n",
    "\n",
    "HUL_train_n *= np.sqrt(Lambda[np.newaxis, np.newaxis, :])\n",
    "HUL_train_compl_tmp = HUL_train_n.reshape(na * nc, nTrain, order='F').T\n",
    "HUL_train_compl_tmp_mean = np.mean(HUL_train_compl_tmp, axis=0)\n",
    "HUL_train_compl = HUL_train_compl_tmp - HUL_train_compl_tmp_mean"
   ],
   "id": "f7cf3d225b077bcf",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:51:11.103800Z",
     "start_time": "2024-11-29T18:51:11.030312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Downlink Testing Data\n",
    "HDL_test_n = H_test.copy()\n",
    "Lambda = 1.0 / np.mean(np.abs(HDL_test_n)**2, axis=(0, 1))\n",
    "HDL_test = HDL_test_n * np.sqrt(Lambda).reshape(1, 1, -1, order='F')\n",
    "if snrTest != -1:\n",
    "    for q in range(nTest):\n",
    "        nPower = np.mean(np.abs(H_test[:, :, q])**2) / snrTest\n",
    "        HDL_test_n[:, :, q] += (np.random.randn(na, nc) + 1j * np.random.randn(na, nc)) * \\\n",
    "                               np.sqrt(nPower / 2)\n",
    "    Lambda = 1.0 / np.mean(np.abs(HDL_test_n)**2, axis=(0, 1))\n",
    "\n",
    "HDL_test_n *= np.sqrt(Lambda[np.newaxis, np.newaxis, :])\n",
    "HDL_test_compl_tmp = HDL_test_n.reshape(na * nc, nTest, order='F').T\n",
    "HDL_test_compl = HDL_test_compl_tmp - HUL_train_compl_tmp_mean\n",
    "\n",
    "print(\"Preprocessing complete.\")\n"
   ],
   "id": "initial_id",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:51:23.676773Z",
     "start_time": "2024-11-29T18:51:11.104547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matlab.engine\n",
    "matlab = matlab.engine.start_matlab()\n",
    "\n",
    "coeff_ori = np.array(matlab.pca(HUL_train_compl))"
   ],
   "id": "d1e52180e0189228",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:51:23.689183Z",
     "start_time": "2024-11-29T18:51:23.678682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def func_rho(h_hat, h):\n",
    "    \"\"\"\n",
    "    Calculate the correlation coefficient between the estimated channel (h_hat)\n",
    "    and the actual channel (h).\n",
    "\n",
    "    Parameters:\n",
    "    h_hat (numpy.ndarray): Estimated channel matrix of shape (n, k).\n",
    "    h (numpy.ndarray): Actual channel matrix of shape (n, k).\n",
    "\n",
    "    Returns:\n",
    "    float: Average correlation coefficient rho_h.\n",
    "    \"\"\"\n",
    "    rho_i = 0\n",
    "    n, k = h.shape\n",
    "\n",
    "    for i in range(k):\n",
    "        # Compute the correlation for each column\n",
    "        numerator = abs(np.dot(h_hat[:, i].conj().T, h[:, i]))\n",
    "        denominator = np.linalg.norm(h_hat[:, i]) * np.linalg.norm(h[:, i])\n",
    "        rho_i += numerator / denominator\n",
    "\n",
    "    # Average the correlation coefficients\n",
    "    rho_h = rho_i / k\n",
    "    return rho_h\n",
    "\n",
    "\n",
    "def func_gram_schmidt(V):\n",
    "    \"\"\"\n",
    "    Perform Gram-Schmidt orthogonalization on the columns of V.\n",
    "    The input matrix V (n x k) is replaced by an orthonormal matrix U (n x k) \n",
    "    whose columns span the same subspace as V.\n",
    "\n",
    "    Parameters:\n",
    "    V (numpy.ndarray): Input matrix of shape (n, k).\n",
    "\n",
    "    Returns:\n",
    "    U (numpy.ndarray): Orthonormal matrix of shape (n, k).\n",
    "    \"\"\"\n",
    "    n, k = V.shape\n",
    "    U = np.zeros((n, k), dtype=V.dtype)\n",
    "\n",
    "    # Normalize the first column\n",
    "    U[:, 0] = V[:, 0] / np.linalg.norm(V[:, 0])\n",
    "\n",
    "    # Iterate through remaining columns\n",
    "    for i in range(1, k):\n",
    "        U[:, i] = V[:, i]\n",
    "        for j in range(i):\n",
    "            projection = np.dot(U[:, j].conj().T, U[:, i]) / (np.linalg.norm(U[:, j])**2)\n",
    "            U[:, i] -= projection * U[:, j]\n",
    "        U[:, i] /= np.linalg.norm(U[:, i])\n",
    "\n",
    "    return U\n",
    "\n",
    "\n",
    "def func_nmse(h_hat, h):\n",
    "    \"\"\"\n",
    "    Calculate the Normalized Mean Squared Error (NMSE) between the estimated\n",
    "    channel (h_hat) and the actual channel (h).\n",
    "\n",
    "    Parameters:\n",
    "    h_hat (numpy.ndarray): Estimated channel matrix.\n",
    "    h (numpy.ndarray): Actual channel matrix.\n",
    "\n",
    "    Returns:\n",
    "    float: NMSE value.\n",
    "    \"\"\"\n",
    "    nmse_h = (np.linalg.norm(h_hat - h, 'fro') / np.linalg.norm(h, 'fro'))**2\n",
    "    return nmse_h\n",
    "\n",
    "\n",
    "def func_allocate_bits(BTot, importances, zUL_train_entries):\n",
    "    \"\"\"\n",
    "    Allocate bits to minimize the theoretical distortion function.\n",
    "    \n",
    "    Parameters:\n",
    "    BTot (int): Total number of feedback bits.\n",
    "    importances (numpy.ndarray): Variance of principal components.\n",
    "    zUL_train_entries (numpy.ndarray): Training entries, shape (nTrain, C, 2).\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Array of bit allocations for principal components.\n",
    "    \"\"\"\n",
    "    # Number of considered PCs\n",
    "    NP = 1\n",
    "\n",
    "    # Initialize bit allocations\n",
    "    Bs = np.zeros(len(importances), dtype=int)\n",
    "    Bs[0] = 1\n",
    "\n",
    "    # Distortion function (df)\n",
    "    df = np.copy(importances)\n",
    "    kmeans = KMeans(n_clusters=2**Bs[0], n_init=10, random_state=0)\n",
    "    z_entries_first_pc = zUL_train_entries[:, 0, :]  # First PC\n",
    "    kmeans.fit(z_entries_first_pc)\n",
    "    df[0] = np.sum(kmeans.inertia_) / zUL_train_entries.shape[0]\n",
    "    df_new = np.copy(df)\n",
    "\n",
    "    # Main allocation loop\n",
    "    for bTot in range(2, BTot + 1):\n",
    "        if bTot % 100 == 0:\n",
    "            print(f\"Considering bTot: {bTot}\")\n",
    "\n",
    "        for np_idx in range(1, NP + 2):  # np_idx starts from 1\n",
    "            if np_idx == 1:  # For the first PC\n",
    "                if df_new[np_idx - 1] == df[np_idx - 1]:\n",
    "                    try:\n",
    "                        z_entries_pc = zUL_train_entries[:, np_idx - 1, :]\n",
    "                        kmeans = KMeans(n_clusters=2**(Bs[np_idx - 1] + 1), n_init=10, random_state=0)\n",
    "                        kmeans.fit(z_entries_pc)\n",
    "                        df_new[np_idx - 1] = np.sum(kmeans.inertia_) / zUL_train_entries.shape[0]\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in k-means for PC {np_idx}: {e}\")\n",
    "            elif Bs[np_idx - 1] < Bs[np_idx - 2]:  # For other PCs\n",
    "                if df_new[np_idx - 1] == df[np_idx - 1]:\n",
    "                    try:\n",
    "                        z_entries_pc = zUL_train_entries[:, np_idx - 1, :]\n",
    "                        kmeans = KMeans(n_clusters=2**(Bs[np_idx - 1] + 1), n_init=10, random_state=0)\n",
    "                        kmeans.fit(z_entries_pc)\n",
    "                        df_new[np_idx - 1] = np.sum(kmeans.inertia_) / zUL_train_entries.shape[0]\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in k-means for PC {np_idx}: {e}\")\n",
    "\n",
    "        # Improvement in distortion\n",
    "        improvement = df - df_new\n",
    "        assert np.min(improvement) >= 0, \"Improvement should not be negative.\"\n",
    "\n",
    "        # Find the PC with maximum improvement\n",
    "        iMax = np.argmax(improvement)\n",
    "        Bs[iMax] += 1\n",
    "        NP = max(NP, iMax + 1)\n",
    "        df[iMax] = df_new[iMax]\n",
    "\n",
    "    # Return only non-zero bit allocations\n",
    "    return Bs[:np.nonzero(Bs)[0][-1] + 1]\n"
   ],
   "id": "20e21ab5d774efa3",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:51:43.652059Z",
     "start_time": "2024-11-29T18:51:23.690004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.fftpack import fftn, ifftn\n",
    "\n",
    "# Reduce Offloading Overhead\n",
    "if reduce_overhead:\n",
    "    print(\"Reducing offloading overhead...\")\n",
    "    coeff = np.zeros_like(coeff_ori)\n",
    "    for i in range(coeff_ori.shape[1]):\n",
    "        # Reshape the i-th principal component\n",
    "        pc = coeff_ori[:, i].reshape((int(np.sqrt(na)), int(np.sqrt(na)), nc))\n",
    "\n",
    "        # Perform FFT\n",
    "        pcDFT = fftn(pc).flatten()\n",
    "\n",
    "        # Create mask for top coefficients\n",
    "        mask = np.zeros_like(pcDFT)\n",
    "        locs = np.argsort(-np.abs(pcDFT))[:int(na * nc / CR)]  # Find top |na*nc/CR| values\n",
    "        mask[locs] = 1\n",
    "\n",
    "        # Apply the mask\n",
    "        pcDFT = pcDFT * mask\n",
    "\n",
    "        # Perform inverse FFT\n",
    "        pcIFFT = ifftn(pcDFT.reshape((int(np.sqrt(na)), int(np.sqrt(na)), nc)))\n",
    "        coeff[:, i] = pcIFFT.flatten()\n",
    "\n",
    "    # Orthogonalize using Gram-Schmidt\n",
    "    coeff = np.array(matlab.func_gram_schmidt(coeff[:, :500]))  # TODO Py gram_schmidt not working, matlab.func_gram_schmidt is working\n",
    "else:\n",
    "    coeff = coeff_ori"
   ],
   "id": "5b672343173d6593",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:52:06.198543Z",
     "start_time": "2024-11-29T18:51:43.652853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Learn Quantization (k-means clustering training)\n",
    "\n",
    "matlab.rng(69)\n",
    "matlab.warning('off', 'stats:kmeans:FailedToConverge')\n",
    "\n",
    "if quantization:\n",
    "    print(\"Training k-means clustering...\")\n",
    "\n",
    "    # Project data using PCA\n",
    "    zUL_train = np.dot(HUL_train_compl, coeff)\n",
    "    zUL_train_entries = np.stack((np.real(zUL_train), np.imag(zUL_train)), axis=-1)\n",
    "\n",
    "    # Calculate importances (variance of each PCA component)\n",
    "    importances = np.var(zUL_train, axis=0)\n",
    "\n",
    "    # Allocate bits for quantization\n",
    "    Bs = np.squeeze(np.array(\n",
    "        matlab.func_allocate_bits(BTot, importances, zUL_train_entries),\n",
    "        dtype=np.int16\n",
    "    ))\n",
    "    C = len(Bs)\n",
    "\n",
    "    # Scale data for k-means clustering\n",
    "    zUL_train_entries_scaled = np.zeros((nTrain, C, 2))\n",
    "    for i in range(C):\n",
    "        zUL_train_entries_scaled[:, i, :] = zUL_train_entries[:, i, :] / np.sqrt(importances[i])\n",
    "\n",
    "    # Determine the number of samples for k-means\n",
    "    nTrainKMeans = min(nTrain, round(1e5 / C))\n",
    "    zUL_train_entriesCSCG = zUL_train_entries_scaled[:nTrainKMeans, :C, :].reshape(-1, 2, order='F')\n",
    "\n",
    "    # Train k-means for different bit levels\n",
    "    quantLevelsCSCG = [None] * Bs[0]\n",
    "    for i in tqdm(range(1, Bs[0] + 1)):\n",
    "        quantLevelsCSCG[i-1] = np.array(\n",
    "            matlab.kmeans(zUL_train_entriesCSCG,2**i, nargout=2)[1]   # Get cluster centers\n",
    "        )\n",
    "        # kmeans = KMeans(n_clusters=2**i, n_init=10)\n",
    "        # kmeans.fit(zUL_train_entriesCSCG)\n",
    "        # quantLevelsCSCG[i - 1] = kmeans.cluster_centers_\n",
    "\n",
    "    # Scale quantization levels back\n",
    "    quantLevels = [None] * C\n",
    "    for i in range(C):\n",
    "        quantLevels[i] = quantLevelsCSCG[Bs[i] - 1] * np.sqrt(importances[i])\n"
   ],
   "id": "dc374a700e9b8789",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:52:07.403956Z",
     "start_time": "2024-11-29T18:52:06.199814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Testing\n",
    "print(\"Testing...\")\n",
    "\n",
    "coeff_trunc = coeff[:, :C]\n",
    "zDL = np.dot(HDL_test_compl, coeff_trunc)\n",
    "\n",
    "if quantization:\n",
    "    for i in range(zDL.shape[0]):\n",
    "        for j in range(zDL.shape[1]):\n",
    "            # Find the closest quantization level\n",
    "            distances = np.abs(zDL[i, j] - (quantLevels[j][:, 0] + 1j * quantLevels[j][:, 1]))\n",
    "            vecIdx = np.argmin(distances)\n",
    "            zDL[i, j] = quantLevels[j][vecIdx, 0] + 1j * quantLevels[j][vecIdx, 1]\n",
    "\n",
    "# Reconstruct the downlink channel\n",
    "HDL_reconst_tmp = np.matmul(zDL, coeff_trunc.conj().T)\n",
    "HDL_reconst = HDL_reconst_tmp + HUL_train_compl_tmp_mean\n",
    "HDL_ori_reconst = HDL_reconst.T.reshape(na, nc, nTest, order='F')"
   ],
   "id": "6607ee0387619e44",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:52:07.864874Z",
     "start_time": "2024-11-29T18:52:07.406261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assessing performance\n",
    "print(\"Assessing performance...\")\n",
    "\n",
    "nmse = np.zeros(nTest)\n",
    "rho = np.zeros(nTest)\n",
    "\n",
    "for i in range(nTest):\n",
    "    ch = HDL_test[:, :, i]\n",
    "    ch_h = HDL_ori_reconst[:, :, i]\n",
    "    nmse[i] = func_nmse(ch_h, ch)  # Assumes func_nmse is defined\n",
    "    rho[i] = func_rho(ch_h, ch)    # Assumes func_rho is defined\n",
    "\n",
    "# Plotting results\n",
    "print(\"Plotting results...\")\n",
    "LineW = 1.5\n",
    "\n",
    "plt.figure()\n",
    "cdf_nmse = np.sort(10 * np.log10(nmse))\n",
    "cdf_rho = np.sort(10 * np.log10(1 - rho))\n",
    "\n",
    "probabilities = np.arange(1, len(cdf_nmse) + 1) / len(cdf_nmse)\n",
    "\n",
    "plt.plot(cdf_nmse, probabilities, label='CDF 10log(NMSE)', linewidth=LineW)\n",
    "plt.plot(cdf_rho, probabilities, label='CDF 10log(1-RHO)', linewidth=LineW)\n",
    "\n",
    "# plt.xlim([-22, 0])\n",
    "# plt.xticks(range(-22, 1, 2))\n",
    "plt.xlabel('Metric (dB)')\n",
    "plt.ylabel('CDF')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Saving results\n",
    "np.save(f'nmse-BTot{BTot}-CR{CR}.npy', nmse)\n",
    "np.save(f'rho-BTot{BTot}-CR{CR}.npy', rho)\n"
   ],
   "id": "220f349ecc3a098c",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:52:07.965828Z",
     "start_time": "2024-11-29T18:52:07.865829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "t = HDL_test[:,:,0]\n",
    "t2 = HDL_ori_reconst[:,:,0]\n",
    "\n",
    "plt.imshow(np.abs(t))\n",
    "plt.show()\n",
    "plt.imshow(np.abs(t2))\n",
    "plt.show()\n"
   ],
   "id": "3158706a2c6440e1",
   "execution_count": 11,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
